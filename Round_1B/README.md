# ğŸ¤– Adobe Hackathon 2025 â€” Round 1B
## ğŸ§¬ Persona-Driven Document Intelligence

This project solves the 1B challenge using semantic embeddings to extract user-specific content from PDFs based on personas and goals.

---

### ğŸ›  Technologies
- Python
- FAISS for vector similarity search
- Sentence Transformers (Embeddings)
- Docker

---

### ğŸ“‚ Folder Structure

Round_1B/
â”œâ”€â”€ run.py
â”œâ”€â”€ layout_parser.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ input/
â”‚ â”œâ”€â”€ challenge1b_input.json
â”‚ â””â”€â”€ pdfs/*.pdf
â”œâ”€â”€ cache/
â”‚ â”œâ”€â”€ embeddings.npz
â”‚ â””â”€â”€ faiss.index
â”œâ”€â”€ Dockerfile

---

### ğŸš€ Run Instructions

Docker:
```bash
docker build -t adobe-round1b .
docker run -v $(pwd)/input:/app/input -v $(pwd)/output:/app/output round1b

Local:bash
pip install -r requirements.txt
python run.py


ğŸ§© Input Format
{
  "persona": "Health-Conscious",
  "goal": "Find low-fat breakfast options",
  "documents": ["Breakfast Ideas.pdf", "Dinner Ideas - Mains_1.pdf"]
}

ğŸ“¤ Output Format
{
  "persona": "Health-Conscious",
  "goal": "Find low-fat breakfast options",
  "matches": [
    {
      "document": "Breakfast Ideas.pdf",
      "text": "Oatmeal with fruits",
      "score": 0.89
    }
  ]
}
âš™ï¸ How It Works
PDFs are parsed using layout_parser.py

Sentences are embedded using pretrained Sentence Transformers

FAISS indexes embeddings for fast similarity search

The best matching sentences are returned per persona-goal pair